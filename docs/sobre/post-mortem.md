# Post Mortem
## Descrição
Post Mortem é utilizada no contexto de projetos de software para se referir a uma análise retrospectiva feita após a conclusão do projeto. Um "post-mortem" tem o objetivo de avaliar o que correu bem durante o projeto, identificar áreas de melhoria e, em última instância, aprender lições valiosas para projetos futuros.

# O que aprendi
| Membro      | Melhorias                          |
| ----------- | ------------------------------------ |
| Caio        |[- Utilização de scripts de extração de dados utilizando Python Ex: Selenium <br/> - Inserção de dados dentro de um banco de dados <br/> - Consulta de dados dentro de um banco de dados <br/> - Criação de dashboards interativas no PowerBi <br/> - Utilização de ferramentas de extração de dados usando Python <br/>- Criação de modelos de pipeline dados no Azure](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues?q=+is%3Aissue+assignee%3Acaiozim112+)|
|Eduardo|[- Elaboração de pipelines <br/> - Extração de dados com python usando Beautiful Soup <br/> - Orquestração de scripts com gitactions  <br/> - Manipulação de instâncias em nuvem <br/> - Tratamento de dados com SQL <br/> - Implementação de um Data Wharehouse <br/> - Visualização de dados com Power BI e Linguagem DAX <br/> - Criação de protótipo com o figma <br/> - Elicitação de requisitos com o cliente real <br/> - Repassar o conhecimento através de tutoriais](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues?q=is%3Aissue+assignee%3AEduardoGurgel+is%3Aclosed)      |
|Pedro|      |
|Victorio|[- Elaboração de pipelines com Azure Data Factory <br/> - Extração de dados com python usando Beautiful Soup, requests, requests-html <br/> - Orquestração de scripts com gitactions  <br/>  - Tratamento de dados com SQL <br/> - Leitura e tratamento de arquivos excel com pandas <br/> - Visualização de dados com Power BI e Linguagem DAX <br/> - Elicitação e descoberta de requisitos com cliente real <br/> - Verificação e validação de requisitos com cliente real](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues?q=+is%3Aissue+assignee%3Avictor-oss+) |
|Ugor|      |

# Contribuição no Projeto
| Membro      | Contribuições|
| ----------- | ------------------------------------ |
|Caio |- Criação de Requisitos <br/>- Mediação das reuniões com o TCU<br/>- Procurar fontes de relevância para o projeto<br/>- Documentação no Github Pages de atas Ex: Elicitação,Fonte de Dados<br/>- Extração de Dados EX: Irena,ONS<br/>- Criações e refatorações da Dashboard do Projeto|
|Eduardo| - [Exportação do banco de dados](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/199) <br/> - [Tutoriais Azure, SQL e Banco de dados](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/197) <br/> - [Scripts de extração e carga](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/pull/177) <br/> - [Publicação da Dashboard](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/174) <br/> - [Pipeline Datafactory](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/171) <br/> - [Carga de dados SQL](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/146) <br/> - [Protótpio Dashbard](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/39) <br/> - [Implementar e evoluir a Dashboard](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/131) <br/> - [Modelagem e elicitação de requisitos](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/7) <br/> - [Mapeamento dos dados](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/4) <br/> - [Orquestração dos dados](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/191) <br/> - [MVP do projeto](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/139) <br/> - [Diagrama de fluxo](https://github.com/ResidenciaTICBrisa/04_PipelineTCU/issues/18)  |
|Pedro|      |
|Victorio| - Criação de Requisitos <br/> - Elicitação e descoberta de fontes de dados para o projeto <br/> - Extração e tratamento de dados Ex: PNE 2050, BEN, Anuário Estatístico de Energia Elétrica, Anuário Estatístico da ANP, PCBIO  <br/>  - Pipeline Datafactory <br/> - Carga de dados SQL <br/> - Implementação da Dashboard <br/> - Orquestração da execução dos scripts pelo Git Actions |
|Ugor|      |

# O que deveria ter feito melhor
| Membro      | Melhorias                          |
| ----------- | ------------------------------------ |
| Caio        | - Entender melhor o problema do cliente <br/>- Conversar melhor com outros membros sobre solução de problemas<br/>- Focar em issues de maior impacto dentro do projeto<br/>- Pegar uma maior quantidade de issues <br/>|
|Eduardo      | - Poderia comparecer mais nas daylies <br/> - Conciliar melhor o projeto com trabalho e faculdade <br/> - Poderia ter escolhido uma opção Open Source <br/> - Distribuir melhor as demandas com os outros membros <br/> - Orientar/comunicar melhor com os outros membros. <br/> - Optar por fazer tutoriais ao vivo ao invés de vídeo.    |
|Pedro|      |
|Victorio|  - Ser mais assertivo na hora de elicitar o problema do cliente <br/> - Me comunicar melhor com a equipe    |
|Ugor|      |

# Quais foram as fraquezas e como aprimorá-las
| Membro      | Fraquezas                          |Aprimoramento|
| ----------- | ------------------------------------ |---------------------|
| Caio        |  **Procrastinação**: Adiar tarefas importantes.<br/>**Gestão do Tempo**: Má administração do tempo, resultando em atrasos.<br/>**Impaciência**: Dificuldade em lidar com situações que exigem paciência.|**Procrastinação**: Estabelecer metas claras, criar um cronograma, e dividir grandes tarefas em partes menores.<br/>**Gestão do Tempo**: Utilizar técnicas de gerenciamento de tempo, como a técnica Pomodoro, estabelecer prioridades claras<br/>**Impaciência:** Praticar a paciência em situações cotidianas, visualizar objetivos a longo prazo e entender que o progresso leva tempo.|
|Eduardo| **Gestão de tempo** Conciliar o projeto com trabalho/faculdade/estágio <br/> **Comunicação**: Comunicar com o grupo atividades que estou fazendo/repassar atividades   | **Gestão de tempo**: Poderia ter produzido mais <br/> **Comunicação**: Repassar mais tarefas que acabei fazendo.    |
|Pedro|      |      |
|Victorio|  **Comunicação com o cliente**: Não entender o problema do cliente e esperar o cliente fornecer os requisitos do projeto <br/> **Comunicação**: Comunicar com o grupo atividades que estou fazendo/repassar atividades  |  **Comunicação com o cliente**: Entender o problema do cliente e não se deixar desviar por sugestões pouco realistas do cliente sobre o que deve ser o projeto <br/> **Comunicação**: Repassar mais tarefas que fiz |
|Ugor|      |      |

# Retrospectiva

## Relato de Experiência - Caio 

Durante o desenvolvimento do projeto, tive a oportunidade de ampliar meus horizontes ao atuar como Community Manager, explorando uma área na qual nunca havia mergulhado anteriormente. Essa experiência permitiu-me entender perspectivas diferentes das minhas habituais e proporcionou contribuições significativas à comunidade externa.

Ao desempenhar o papel de Community Manager, enfrentei desafios intrigantes, especialmente no uso de ferramentas de extração de dados. Inicialmente, encontrei dificuldades para resolver esses problemas de maneira eficiente. No entanto, através de conversas e colaboração com outros membros da equipe, conseguimos não apenas superar esses obstáculos, mas também encontrar soluções ideais que beneficiaram todo o projeto.

Essa jornada não apenas me proporcionou um aprendizado valioso em uma nova área, mas também destacou a importância da colaboração e da troca de conhecimento dentro da comunidade.

Com o aprendizado do projeto, consegui uma vaga de estágio na Agência de Aviação Civil (ANAC), atuando como analista de dados e utilizando ferramentas semelhantes às que usamos durante o desenvolvimento do projeto.

## Relato de Experiência - Eduardo
O projeto foi fundamental para o meu aprendizado, consegui desenvolver habilidades variadas, enfrentar desafios da rotina e colocar em prática todo conhecimento teórico que adquiri durante o curso de Engenharia de Software.

Aprendi a configurar serviços, gerenciar instancias virtuais e utilizar o azure para armazenar e processar dados. Também aprendi a projetar e otimizar bancos de dados, executar consultas e manter a integridade dos dados. Todo esse conhecimento adquirido foi muito importante para mim, pois antes de começar o projeto eu não tinha ideia do que se tratava um (ETL), então foi um desafio que acabei superando,e repassando meu conhecimento para os outros integrantes do grupo através de tutoriais.

Como Product Owner, meu progresso ao longo do projeto foi marcada por uma evolução contínua e desafiadora. Principalmente para entender o que o cliente desejava, esse foi um desafio que demandou um tempo considerável e um esforço significativo da equipe. No entanto, nós acabamos definindo o MVP e estabelecendo melhorias para o produto, o que foi fundamental para uma melhor divisão de tarefas e finalização do projeto.

No geral, o projeto desempenhou um papel fundamental em minha trajetória de desenvolvimento profissional, espero muito que seja uma ferramenta útil no contexto da transição energética brasileira, e sem dúvida será um destaque em meu portfólio. Além disso, muito dos conceitos que eu vi durante o projeto foram fundamentais para que eu conseguisse receber algumas propostas de trabalho e também ser aprovado no concurso da Dataprev.


## Relato de Experiência - Pedro 


## Relato de Experiência - Victorio 
Esta jornada me proporcionou um novo olhar sobre o potencial dos dados das fontes de dados, governamentais ou não, disponíveis na internet e a importância de integrar esses dados em uma única plataforma.

Desenvolvi scripts personalizados para recuperar os dados necessários. Essa etapa exigiu uma compreensão sólida dos formatos de dados específicos e autenticação. Uma vez que os dados foram coletados, realizei limpeza e pré-processamento para garantir que estivessem prontos para as próximas fases.

O principal do projeto foi a implementação de um pipeline ETL (Extração, Transformação e Carga) usando Azure Data Factory, automatizando a extração regular de dados. Com os dados preparados e armazenados em um repositório, criamos visualizações significativas para tornar as informações acessíveis e úteis usando o Power BI.

Essa experiência solidificou meu conhecimento em ciência de dados, e estou ansioso para explorar novos projetos e desafios. A experiência também me garantiu mais conhecimento na hora de elicitar os problemas de um cliente real.

## Relato de Experiência - Ugor 


